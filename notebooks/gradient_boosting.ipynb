{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab73bf1a",
   "metadata": {},
   "source": [
    "# Градиентный бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303f04b9",
   "metadata": {},
   "source": [
    "Градиентный спуск (Gradient Descent)\n",
    "\n",
    "Это метод оптимизации: ты берёшь параметры одной модели (например, веса нейросети) и двигаешься по градиенту вниз, чтобы уменьшить ошибку.\n",
    "\n",
    "Ассоциация: спуск с горы — каждый шаг ты делаешь туда, где “ниже” (меньше loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42587efe",
   "metadata": {},
   "source": [
    "# Градиентный спуск "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd31ab8",
   "metadata": {},
   "source": [
    "Градиентный бустинг\n",
    "\n",
    "Это модель/алгоритм ансамбля, который тоже использует идею градиента, но не по весам одной модели, а в “пространстве функций”: мы добавляем новые деревья как “маленькие поправки”, уменьшающие loss. Это классическое описание у Фридмана.\n",
    "\n",
    "Ассоциация:\n",
    "представь, что ты пишешь текст, а потом его по очереди правят 100 редакторов.\n",
    "1-й редактор исправил очевидные ошибки, 2-й — то, что осталось, 3-й — ещё тоньше… В итоге получается сильный результат."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0ea893",
   "metadata": {},
   "source": [
    "# Отличие бустинга от спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a9ba1f",
   "metadata": {},
   "source": [
    "мы строим не одну “большую умную” модель, а ансамбль из многих простых моделей (обычно небольших деревьев).\n",
    "Модели добавляются по очереди: каждая новая пытается исправить ошибки предыдущих."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8cfe57",
   "metadata": {},
   "source": [
    "XGBoost и CatBoost библиотеки которые осущ град бустинг на деревьях"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7396d5f0",
   "metadata": {},
   "source": [
    "больше инфы: \n",
    "\n",
    "**XGBoost**\n",
    "\n",
    "Это библиотека, которая “делает градиентный бустинг на деревьях” очень эффективно (скорость/оптимизации/масштабирование). В их документации прямо сказано, что XGBoost — это “optimized distributed gradient boosting library”, и что это бустинг в рамках Gradient Boosting framework.\n",
    "\n",
    "Ассоциация: “градиентный бустинг на стероидах” — тот же алгоритм по идее, но очень круто оптимизированный.\n",
    "\n",
    "**CatBoost**\n",
    "\n",
    "Это тоже градиентный бустинг на деревьях, но сделанный так, чтобы очень хорошо работать с категориальными признаками (типа “город”, “пол”, “профессия”) и уменьшать переобучение/утечки при их обработке. Это прямо заявлено в их статье и документации.\n",
    "\n",
    "Ассоциация: XGBoost = “быстрый мощный бустинг”, CatBoost = “бустинг, который особенно удобен, когда много категорий”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0d7ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaburtseva/Documents/проект грант/slr-env/.venv/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  raw_prediction = X @ weights + intercept\n",
      "/Users/mariaburtseva/Documents/проект грант/slr-env/.venv/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  raw_prediction = X @ weights + intercept\n",
      "/Users/mariaburtseva/Documents/проект грант/slr-env/.venv/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  raw_prediction = X @ weights + intercept\n",
      "/Users/mariaburtseva/Documents/проект грант/slr-env/.venv/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:212: RuntimeWarning: divide by zero encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Users/mariaburtseva/Documents/проект грант/slr-env/.venv/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:212: RuntimeWarning: overflow encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Users/mariaburtseva/Documents/проект грант/slr-env/.venv/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:212: RuntimeWarning: invalid value encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Users/mariaburtseva/Documents/проект грант/slr-env/.venv/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:333: RuntimeWarning: divide by zero encountered in matmul\n",
      "  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n",
      "/Users/mariaburtseva/Documents/проект грант/slr-env/.venv/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:333: RuntimeWarning: overflow encountered in matmul\n",
      "  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n",
      "/Users/mariaburtseva/Documents/проект грант/slr-env/.venv/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:333: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Baseline(LogReg) ===\n",
      "Accuracy: 0.9504\n",
      "F1 weighted: 0.9502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9515    0.9691    0.9602       162\n",
      "           1     0.9485    0.9200    0.9340       100\n",
      "\n",
      "    accuracy                         0.9504       262\n",
      "   macro avg     0.9500    0.9446    0.9471       262\n",
      "weighted avg     0.9503    0.9504    0.9502       262\n",
      "\n",
      "\n",
      "=== GradientBoosting(HistGB) ===\n",
      "Accuracy: 0.9466\n",
      "F1 weighted: 0.9463\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9458    0.9691    0.9573       162\n",
      "           1     0.9479    0.9100    0.9286       100\n",
      "\n",
      "    accuracy                         0.9466       262\n",
      "   macro avg     0.9468    0.9396    0.9429       262\n",
      "weighted avg     0.9466    0.9466    0.9463       262\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaburtseva/Documents/проект грант/slr-env/.venv/lib/python3.11/site-packages/sklearn/utils/extmath.py:227: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/mariaburtseva/Documents/проект грант/slr-env/.venv/lib/python3.11/site-packages/sklearn/utils/extmath.py:227: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/mariaburtseva/Documents/проект грант/slr-env/.venv/lib/python3.11/site-packages/sklearn/utils/extmath.py:227: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "\n",
    "# 1) Загружаем Titanic (OpenML)\n",
    "X, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n",
    "\n",
    "# y может быть строками ('0'/'1') — приведём к int 0/1\n",
    "y = y.astype(int)\n",
    "\n",
    "# 2) Разделим на train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 3) Препроцессинг: числовые и категориальные колонки\n",
    "num_cols = X_train.select_dtypes(include=[\"number\"]).columns\n",
    "cat_cols = X_train.select_dtypes(exclude=[\"number\"]).columns\n",
    "\n",
    "numeric_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "])\n",
    "\n",
    "categorical_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipe, num_cols),\n",
    "        (\"cat\", categorical_pipe, cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# 4) Baseline: Logistic Regression\n",
    "baseline = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    (\"model\", LogisticRegression(max_iter=2000))\n",
    "])\n",
    "\n",
    "# 5) Gradient Boosting (быстрый вариант в sklearn)\n",
    "gb = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    (\"model\", HistGradientBoostingClassifier(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=3,\n",
    "        max_iter=400,\n",
    "        early_stopping=True,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 6) Обучаем\n",
    "baseline.fit(X_train, y_train)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# 7) Оцениваем\n",
    "for name, model in [(\"Baseline(LogReg)\", baseline), (\"GradientBoosting(HistGB)\", gb)]:\n",
    "    pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    f1w = f1_score(y_test, pred, average=\"weighted\")\n",
    "    print(\"\\n===\", name, \"===\")\n",
    "    print(\"Accuracy:\", round(acc, 4))\n",
    "    print(\"F1 weighted:\", round(f1w, 4))\n",
    "    print(classification_report(y_test, pred, digits=4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
